{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import platform\n",
    "\n",
    "from numpy.f2py.auxfuncs import throw_error\n",
    "\n",
    "from utils import set_seed, create_directory, display_images\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import timm # For CoAtNet\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SEED = 111\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "9771082fcdd0cc74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download dataset if not available\n",
    "data_path = create_directory(\"data\")\n",
    "import kagglehub\n",
    "\n",
    "# https://github.com/Kaggle/kagglehub/issues/175\n",
    "os.environ['KAGGLEHUB_CACHE'] = data_path\n",
    "# Download latest version\n",
    "utkface_data = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
    "\n",
    "# Thanks windows, cant get the full path due to limit\n",
    "utkface_data_path = os.path.join(data_path, \"datasets\", \"jangedoo\", \"utkface-new\", \"versions\", \"1\", \"UTKFace\")\n",
    "if not os.path.isdir(utkface_data_path):\n",
    "    print(f\"path does is not dir: {utkface_data_path}\")\n"
   ],
   "id": "ec157b1d0206e38d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pre evaluate dataset\n",
    "def get_img_ages(dataset_path, image_paths, ages):\n",
    "    parsed_files_count = 0\n",
    "    skipped_files_count = 0\n",
    "    for filepath in glob.glob(os.path.join(dataset_path, '*.jpg')):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 2:\n",
    "                skipped_files_count += 1\n",
    "                continue\n",
    "            age = int(parts[0])\n",
    "            if not (0 <= age <= 116):\n",
    "                skipped_files_count += 1\n",
    "                continue\n",
    "            image_paths.append(filepath)\n",
    "            ages.append(age)\n",
    "            parsed_files_count +=1\n",
    "        except (ValueError, IndexError):\n",
    "            skipped_files_count += 1\n",
    "\n",
    "    print(f\"Successfully parsed {parsed_files_count} image files.\")\n",
    "    print(f\"Skipped {skipped_files_count} image files due to parsing issues or invalid data.\")\n",
    "\n",
    "def age_to_class(age):\n",
    "    if age < 18: return 0\n",
    "    elif age <= 40: return 1\n",
    "    elif age <= 60: return 2\n",
    "    else: return 3\n",
    "\n",
    "image_paths_all = []\n",
    "ages_all = []\n",
    "get_img_ages(utkface_data_path, image_paths_all, ages_all)\n",
    "\n",
    "if not image_paths_all:\n",
    "    raise FileNotFoundError(f\"No valid JPG images found or parsed in {utkface_data_path}.\")\n",
    "\n",
    "# Create DF\n",
    "df_all = pd.DataFrame({'image_path': image_paths_all, 'age': ages_all})\n",
    "df_all['age_class'] = df_all['age'].apply(age_to_class)\n",
    "class_names_list = ['<18', '18-40', '41-60', '>60']\n",
    "print(\"Class distribution:\\n\", df_all['age_class'].value_counts(normalize=True).sort_index())"
   ],
   "id": "7967d127dbd5bc3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "train_df, test_df = train_test_split(df_all, test_size=TEST_RATIO, random_state=SEED, stratify=df_all['age_class'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=VAL_RATIO / (1 - TEST_RATIO), random_state=SEED, stratify=train_df['age_class'])\n",
    "\n",
    "total_samples = len(df_all[\"image_path\"])\n",
    "count_train = len(train_df)\n",
    "count_val = len(val_df)\n",
    "count_test = len(test_df)\n",
    "\n",
    "percent_train = (count_train / total_samples) * 100\n",
    "percent_val = (count_val / total_samples) * 100\n",
    "percent_test = (count_test / total_samples) * 100\n",
    "print(f\"Total samples:      {total_samples}\")\n",
    "print(f\"Training samples:   {count_train} ({percent_train}%)\")\n",
    "print(f\"Validation samples: {count_val}   ({percent_val}%)\")\n",
    "print(f\"Test samples:       {count_test}  ({percent_test}%)\")\n"
   ],
   "id": "9a9d5d7dbdd044af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Taken from https://github.com/hamkerlab/DL_for_practitioners/blob/main/06_1_SSL_SimCLR/06_1_SSL_SimCLR.ipynb\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, use_bias=True, use_bn=False, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        self.linear = nn.Linear(self.in_features, self.out_features, bias=self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.BatchNorm1d(self.out_features)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Taken from https://github.com/hamkerlab/DL_for_practitioners/blob/main/06_1_SSL_SimCLR/06_1_SSL_SimCLR.ipynb\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, head_type='nonlinear', **kwargs):\n",
    "        super(ProjectionHead, self).__init__(**kwargs)\n",
    "        # ... (same as before) ...\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.head_type = head_type\n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features, self.out_features, False, True)\n",
    "        elif self.head_type == 'nonlinear': # Standard for SimCLR\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features, self.hidden_features, True, True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(self.hidden_features, self.out_features, False, True)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Taken from https://github.com/hamkerlab/DL_for_practitioners/blob/main/06_1_SSL_SimCLR/06_1_SSL_SimCLR.ipynb\n",
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self._mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def _mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, diagonal=self.batch_size) #torch.diag(input = sim, diagonal = self.batch_size)\n",
    "        sim_j_i = torch.diag(input = sim, diagonal =-self.batch_size)\n",
    "\n",
    "        # We have 2N samples\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "\n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss\n"
   ],
   "id": "4bb132543fa50156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class UTKFaceSimCLRDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_size, s_jitter=0.5, is_train=True):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # SimCLR Augmentation\n",
    "        if is_train:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(size=image_size, scale=(0.2, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.ColorJitter(brightness=0.8*s_jitter, contrast=0.8*s_jitter,\n",
    "                                           saturation=0.8*s_jitter, hue=0.2*s_jitter)\n",
    "                ], p=0.8),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.GaussianBlur(kernel_size=max(3, image_size//20*2+1), sigma=(0.1, 2.0))\n",
    "                ], p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=NORMALIZE_STD)\n",
    "            ])\n",
    "        else: # Minimal augmentation for validation because still need two views for SimCLR loss\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(size=image_size, scale=(0.8, 1.0)), # Less aggressive crop\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomGrayscale(p=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD)\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = int(self.dataframe.iloc[idx]['age_class'])\n",
    "\n",
    "        x_i = self.transform(image)\n",
    "        x_j = self.transform(image)\n",
    "        return x_i, x_j, label\n",
    "\n"
   ],
   "id": "7b59da5773fa765e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimCLRCoAtNetModel(nn.Module):\n",
    "    def __init__(self, coatnet_model_name, projection_hidden_features, projection_out_features):\n",
    "        super().__init__()\n",
    "        #  TODO: use num_classes 0?\n",
    "        # num_classes=0 = no need for final classification layer.\n",
    "        self.encoder = timm.create_model(coatnet_model_name, pretrained=False, num_classes=0)\n",
    "\n",
    "        # Get feature dimension from CoAtNet\n",
    "        coanet_feature_dim = self.encoder.num_features\n",
    "        print(f\"CoAtNet ('{coatnet_model_name}') output feature dimension: {coanet_feature_dim}\")\n",
    "\n",
    "        # TODO: why no \"self.latent_layer = LinearLayer(128,self.num_features, True, True)\" ?\n",
    "        # 2. Projection Head\n",
    "        self.projector = ProjectionHead(\n",
    "            in_features=coanet_feature_dim,\n",
    "            hidden_features=projection_hidden_features, # e.g., coanet_feature_dim or a fixed value like 2048\n",
    "            out_features=projection_out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x) # Features from CoAtNet: [batch_size, coanet_feature_dim]\n",
    "        if h.ndim > 2: # Should already be [B, C] from timm with num_classes=0\n",
    "            h = torch.squeeze(h)\n",
    "        z = self.projector(h) # Projected features: [batch_size, projection_out_features]\n",
    "        return z # For SimCLR pre-training, only z is needed for loss calculation"
   ],
   "id": "d3b3a7fde2df46de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = UTKFaceSimCLRDataset(train_df, IMAGE_SIZE, s_jitter=0.5, is_train=True)\n",
    "val_dataset = UTKFaceSimCLRDataset(val_df, IMAGE_SIZE, is_train=False)\n",
    "test_dataset = UTKFaceSimCLRDataset(test_df, IMAGE_SIZE, is_train=False)\n",
    "\n",
    "num_workers = 2\n",
    "if platform.system() == \"Windows\":\n",
    "    num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True if DEVICE.type == 'cuda' else False)\n"
   ],
   "id": "4c5085342e2a6f1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# list available CoAtNet models\n",
    "timm.list_models(\"CoAtNet*\", pretrained=False)"
   ],
   "id": "eb69602599f1ace1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "COATNET_MODEL_NAME = \"coatnet_0_224\"\n",
    "PROJECTION_DIM = 128\n",
    "\n",
    "model = SimCLRCoAtNetModel(\n",
    "    coatnet_model_name=COATNET_MODEL_NAME,\n",
    "    projection_hidden_features=512,\n",
    "    projection_out_features=PROJECTION_DIM\n",
    ").to(DEVICE)"
   ],
   "id": "f46fc79aeb81f1da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TEMPERATURE = 0.1\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "EPOCHS = 50\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "criterion = SimCLR_Loss(batch_size=BATCH_SIZE, temperature=TEMPERATURE).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#     optimizer,\n",
    "#     lambda epoch: (epoch + 1) / WARMUP_EPOCHS if WARMUP_EPOCHS > 0 else 1.0,\n",
    "#     verbose=True) # Ensure it stays 1.0 after warmup\n",
    "\n",
    "# TODO: check https://github.com/hamkerlab/DL_for_practitioners/blob/c80d72b77250a7dee47a9e79182af424faffedea/04_1_ViT/04_1_WarmUpScheduler.ipynb#L71 5. Recommended Warmup Settings for Different Scenarios\n",
    "# Ramps LR with a small value\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "                            optimizer,\n",
    "                            start_factor=1.0 / WARMUP_EPOCHS if WARMUP_EPOCHS > 0 else 1.0,\n",
    "                            end_factor=1.0,\n",
    "                            total_iters=WARMUP_EPOCHS)\n",
    "\n",
    "# Cosine Decay (NO restarts for a single run)\n",
    "# eta_min=0 -> minimum learning rate\n",
    "scheduler_cosine_decay = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                            optimizer,\n",
    "                            T_max=EPOCHS - WARMUP_EPOCHS,\n",
    "                            eta_min=0, # TODO: Why 0?\n",
    "                            last_epoch=-1)\n",
    "\n",
    "combined_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer,\n",
    "                                  schedulers=[scheduler_warmup, scheduler_cosine_decay],\n",
    "                                  milestones=[WARMUP_EPOCHS])\n",
    "\n"
   ],
   "id": "52304ca3237a159a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Training Loop ---\n",
    "print(f\"Starting SimCLR pre-training with {COATNET_MODEL_NAME} for {EPOCHS} epochs...\")\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "PATH_TO_BEST_ENCODER_WEIGHTS = f'./{COATNET_MODEL_NAME}_simclr_encoder_best_val_loss.pth'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    #  Training Phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    pbar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n",
    "    for x_i, x_j, _ in pbar_train:\n",
    "        x_i, x_j = x_i.to(DEVICE), x_j.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        z_i = model(x_i)\n",
    "        z_j = model(x_j)\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        pbar_train.set_postfix({\"Loss\": loss.item()})\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    #  Validation Phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n",
    "        for x_i, x_j, _ in pbar_val:\n",
    "            x_i, x_j = x_i.to(DEVICE), x_j.to(DEVICE)\n",
    "            z_i = model(x_i)\n",
    "            z_j = model(x_j)\n",
    "            loss = criterion(z_i, z_j)\n",
    "            total_val_loss += loss.item()\n",
    "            pbar_val.set_postfix({\"Loss\": loss.item()})\n",
    "    avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Scheduler step\n",
    "    combined_scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_time_taken = (time.time() - epoch_start_time) / 60\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - LR: {current_lr:.6f} - Time: {epoch_time_taken:.2f} min\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.encoder.state_dict(), PATH_TO_BEST_ENCODER_WEIGHTS)\n",
    "        print(f\"Epoch {epoch+1}: New best SSL validation loss: {avg_val_loss:.4f}. Saved encoder to {PATH_TO_BEST_ENCODER_WEIGHTS}\")\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or (epoch + 1) == EPOCHS:\n",
    "        save_path = f'./{COATNET_MODEL_NAME}_simclr_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.encoder.state_dict(), save_path)\n",
    "        print(f\"Saved pre-trained encoder to {save_path}\")\n",
    "\n",
    "print(\"SimCLR Pre-training finished!\")\n",
    "\n",
    "# Optional: Plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SimCLR Pre-training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{COATNET_MODEL_NAME}_simclr_loss_plot.png')\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "5e9c5ceb2da84317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# taken from https://github.com/hamkerlab/DL_for_practitioners/blob/c80d72b77250a7dee47a9e79182af424faffedea/Utils/plotting.py\n",
    "def visualize_embeddings_tsne(embeddings: np.ndarray | torch.Tensor,\n",
    "                              labels: np.ndarray | torch.Tensor,\n",
    "                              output_dir: str | None, # type hint for output_dir\n",
    "                              class_names: list[str], # type hint for class_names\n",
    "                              n_samples: int = 2000,\n",
    "                              num_components: int = 2,\n",
    "                              title_suffix: str = \"\") -> None: # Added title_suffix\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE # Ensure TSNE is imported here if not globally\n",
    "\n",
    "        # check if data is numpy\n",
    "        if torch.is_tensor(embeddings):\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        if torch.is_tensor(labels):\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "        # Subsample if too many points\n",
    "        if len(embeddings) > n_samples:\n",
    "            print(f\"Subsampling {n_samples} out of {len(embeddings)} for t-SNE.\")\n",
    "            indices = np.random.choice(len(embeddings), n_samples, replace=False)\n",
    "            embeddings = embeddings[indices]\n",
    "            labels = labels[indices]\n",
    "\n",
    "        # Apply t-SNE\n",
    "        print(\"Applying t-SNE... (this may take a while)\")\n",
    "        tsne = TSNE(n_components=num_components, random_state=SEED, perplexity=30, n_iter=1000, init='pca', learning_rate='auto') # Added init and lr\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "        print(\"t-SNE done.\")\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        unique_labels = np.unique(labels)\n",
    "        for class_label_val in unique_labels:\n",
    "            if int(class_label_val) < len(class_names):\n",
    "                 class_name_str = class_names[int(class_label_val)]\n",
    "            else:\n",
    "                 class_name_str = f\"Class {int(class_label_val)}\" # Fallback if class_names is too short\n",
    "\n",
    "            indices = labels == class_label_val\n",
    "            plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=class_name_str, alpha=0.7)\n",
    "\n",
    "        plt.title(f't-SNE Visualization of Encoder Embeddings {title_suffix}')\n",
    "        plt.xlabel(\"t-SNE Component 1\")\n",
    "        plt.ylabel(\"t-SNE Component 2\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        if output_dir is not None:\n",
    "            os.makedirs(output_dir, exist_ok=True) # Ensure dir exists\n",
    "            plt.savefig(os.path.join(output_dir, f'embeddings_tsne{title_suffix.replace(\" \", \"_\")}.png'), dpi=300)\n",
    "            print(f\"t-SNE plot saved to {os.path.join(output_dir, f'embeddings_tsne{title_suffix.replace(\" \", \"_\")}.png')}\")\n",
    "            plt.close() # Close plot if saving to file to prevent display issues in loops\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"scikit-learn not installed, skipping embedding visualization for t-SNE.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during t-SNE visualization: {e}\")"
   ],
   "id": "c646e829ba0d1aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nProceeding to t-SNE visualization...\")\n",
    "\n",
    "# Instantiate the encoder architecture\n",
    "feature_extractor_for_tsne = timm.create_model(COATNET_MODEL_NAME, pretrained=False, num_classes=0)\n",
    "\n",
    "# Load the weights of the \"best\" encoder\n",
    "if os.path.exists(PATH_TO_BEST_ENCODER_WEIGHTS):\n",
    "    feature_extractor_for_tsne.load_state_dict(torch.load(PATH_TO_BEST_ENCODER_WEIGHTS, map_location=DEVICE))\n",
    "    print(f\"Loaded best encoder weights from {PATH_TO_BEST_ENCODER_WEIGHTS} for t-SNE.\")\n",
    "    feature_extractor_for_tsne.to(DEVICE)\n",
    "    feature_extractor_for_tsne.eval()\n",
    "\n",
    "    if val_df is not None and not val_df.empty:\n",
    "        tsne_sample_size = len(val_df)  # Number of samples for t-SNE\n",
    "        tsne_df = val_df\n",
    "        print(f\"Using {len(tsne_df)} samples from validation set for t-SNE.\")\n",
    "\n",
    "        # TODO: might need even a more minimal transform!!\n",
    "        feature_dataset = UTKFaceSimCLRDataset(tsne_df, IMAGE_SIZE, is_train=False)\n",
    "        feature_loader = DataLoader(feature_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        all_features = []\n",
    "        all_labels_for_tsne = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels_batch in tqdm(feature_loader, desc=\"Extracting features for t-SNE\"):\n",
    "                images = images.to(DEVICE)\n",
    "                 # Get 'h' from encoder?\n",
    "                features = feature_extractor_for_tsne(images)\n",
    "                all_features.append(features.cpu())\n",
    "                all_labels_for_tsne.append(labels_batch.cpu())\n",
    "\n",
    "        if all_features:\n",
    "            all_features_tensor = torch.cat(all_features, dim=0)\n",
    "            all_labels_tensor = torch.cat(all_labels_for_tsne, dim=0)\n",
    "\n",
    "            visualize_embeddings_tsne(\n",
    "                embeddings=all_features_tensor,\n",
    "                labels=all_labels_tensor,\n",
    "                output_dir=\"./tsne_plots\",\n",
    "                class_names=class_names_list,\n",
    "                n_samples=tsne_sample_size,\n",
    "                title_suffix=f\" after {EPOCHS} Epochs SimCLR\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"No features extracted for t-SNE.\")\n",
    "    else:\n",
    "        print(\"Validation DataFrame (val_df) is empty or None!! \")"
   ],
   "id": "a487f3d51d8cfbd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

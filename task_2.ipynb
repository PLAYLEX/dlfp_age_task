{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9771082fcdd0cc74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of GPUs found: 1\n",
      "\n",
      "GPU 0: NVIDIA A100-SXM4-40GB\n",
      "  ID: 0\n",
      "  Total Memory: 40960MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "from numpy.f2py.auxfuncs import throw_error\n",
    "\n",
    "from utils import set_seed, create_directory, display_images\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import timm # For CoAtNet\n",
    "from tqdm.notebook import tqdm\n",
    "import GPUtil\n",
    "from optimizers import LARS\n",
    "\n",
    "SEED = 111\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "gpus = GPUtil.getGPUs()\n",
    "if not gpus:\n",
    "    print(\"No GPUs found by GPUtil.\")\n",
    "else:\n",
    "    print(f\"Number of GPUs found: {len(gpus)}\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"\\nGPU {i}: {gpu.name}\")\n",
    "        print(f\"  ID: {gpu.id}\")\n",
    "        print(f\"  Total Memory: {gpu.memoryTotal:.0f}MB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec157b1d0206e38d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /home/jupyter/dlfp_age_task/data\n",
      "utk face path /home/jupyter/dlfp_age_task/data/datasets/jangedoo/utkface-new/versions/1/UTKFace\n"
     ]
    }
   ],
   "source": [
    "# Download dataset if not available\n",
    "data_path = create_directory(\"data\")\n",
    "import kagglehub\n",
    "\n",
    "# https://github.com/Kaggle/kagglehub/issues/175\n",
    "os.environ['KAGGLEHUB_CACHE'] = data_path\n",
    "# Download latest version\n",
    "utkface_data = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
    "\n",
    "# Thanks windows, cant get the full path due to limit\n",
    "utkface_data_path = os.path.join(data_path, \"datasets\", \"jangedoo\", \"utkface-new\", \"versions\", \"1\", \"UTKFace\")\n",
    "if not os.path.isdir(utkface_data_path):\n",
    "    print(f\"path does is not dir: {utkface_data_path}\")\n",
    "else:\n",
    "    print(f\"utk face path {utkface_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7967d127dbd5bc3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 23708 image files.\n",
      "Skipped 0 image files due to parsing issues or invalid data.\n",
      "Class distribution:\n",
      " age_class\n",
      "0    0.178547\n",
      "1    0.538510\n",
      "2    0.181837\n",
      "3    0.101105\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Pre evaluate dataset\n",
    "def get_img_ages(dataset_path, image_paths, ages):\n",
    "    parsed_files_count = 0\n",
    "    skipped_files_count = 0\n",
    "    for filepath in glob.glob(os.path.join(dataset_path, '*.jpg')):\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 2:\n",
    "                skipped_files_count += 1\n",
    "                continue\n",
    "            age = int(parts[0])\n",
    "            if not (0 <= age <= 116):\n",
    "                skipped_files_count += 1\n",
    "                continue\n",
    "            image_paths.append(filepath)\n",
    "            ages.append(age)\n",
    "            parsed_files_count +=1\n",
    "        except (ValueError, IndexError):\n",
    "            skipped_files_count += 1\n",
    "\n",
    "    print(f\"Successfully parsed {parsed_files_count} image files.\")\n",
    "    print(f\"Skipped {skipped_files_count} image files due to parsing issues or invalid data.\")\n",
    "\n",
    "def age_to_class(age):\n",
    "    if age < 18: return 0\n",
    "    elif age <= 40: return 1\n",
    "    elif age <= 60: return 2\n",
    "    else: return 3\n",
    "\n",
    "image_paths_all = []\n",
    "ages_all = []\n",
    "get_img_ages(utkface_data_path, image_paths_all, ages_all)\n",
    "\n",
    "if not image_paths_all:\n",
    "    raise FileNotFoundError(f\"No valid JPG images found or parsed in {utkface_data_path}.\")\n",
    "\n",
    "# Create DF\n",
    "df_all = pd.DataFrame({'image_path': image_paths_all, 'age': ages_all})\n",
    "df_all['age_class'] = df_all['age'].apply(age_to_class)\n",
    "class_names_list = ['<18', '18-40', '41-60', '>60']\n",
    "print(\"Class distribution:\\n\", df_all['age_class'].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9d5d7dbdd044af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:      23708\n",
      "Training samples:   16594 (69.9932512232158%)\n",
      "Validation samples: 3557   (15.003374388392105%)\n",
      "Test samples:       3557  (15.003374388392105%)\n"
     ]
    }
   ],
   "source": [
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "train_df, test_df = train_test_split(df_all, test_size=TEST_RATIO, random_state=SEED, stratify=df_all['age_class'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=VAL_RATIO / (1 - TEST_RATIO), random_state=SEED, stratify=train_df['age_class'])\n",
    "\n",
    "total_samples = len(df_all[\"image_path\"])\n",
    "count_train = len(train_df)\n",
    "count_val = len(val_df)\n",
    "count_test = len(test_df)\n",
    "\n",
    "percent_train = (count_train / total_samples) * 100\n",
    "percent_val = (count_val / total_samples) * 100\n",
    "percent_test = (count_test / total_samples) * 100\n",
    "print(f\"Total samples:      {total_samples}\")\n",
    "print(f\"Training samples:   {count_train} ({percent_train}%)\")\n",
    "print(f\"Validation samples: {count_val}   ({percent_val}%)\")\n",
    "print(f\"Test samples:       {count_test}  ({percent_test}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb132543fa50156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/hamkerlab/DL_for_practitioners/blob/main/06_1_SSL_SimCLR/06_1_SSL_SimCLR.ipynb\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, use_bias=True, use_bn=False, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        self.linear = nn.Linear(self.in_features, self.out_features, bias=self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.BatchNorm1d(self.out_features)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Taken from https://github.com/hamkerlab/DL_for_practitioners/blob/main/06_1_SSL_SimCLR/06_1_SSL_SimCLR.ipynb\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, head_type='nonlinear', **kwargs):\n",
    "        super(ProjectionHead, self).__init__(**kwargs)\n",
    "        # ... (same as before) ...\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.head_type = head_type\n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features, self.out_features, False, True)\n",
    "        elif self.head_type == 'nonlinear': # Standard for SimCLR\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features, self.hidden_features, True, True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(self.hidden_features, self.out_features, False, True)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Taken from https://github.com/hamkerlab/DL_for_practitioners/blob/main/06_1_SSL_SimCLR/06_1_SSL_SimCLR.ipynb\n",
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self._mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def _mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        actual_b_size = z_i.shape[0]\n",
    "        if actual_b_size != self.batch_size:\n",
    "            print(f\"WARNING: batch size from z_i ({actual_b_size}) != self.batch_size ({self.batch_size})\")\n",
    "            \n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, diagonal=self.batch_size) #torch.diag(input = sim, diagonal = self.batch_size)\n",
    "        sim_j_i = torch.diag(input = sim, diagonal =-self.batch_size)\n",
    "        \n",
    "        if sim_i_j.nelement() == 0 or sim_j_i.nelement() == 0: # Check if empty\n",
    "            print(\"ERROR: sim_i_j or sim_j_i is empty!\")\n",
    "            print(f\"Shape of sim: {sim.shape}, diagonal for sim_i_j: {actual_b_size}\")\n",
    "\n",
    "        # We have 2N samples\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "\n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b59da5773fa765e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class UTKFaceSimCLRDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_size, s_jitter=0.5, is_train=True):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # SimCLR Augmentation\n",
    "        if is_train:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(size=image_size, scale=(0.2, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.ColorJitter(brightness=0.8*s_jitter, contrast=0.8*s_jitter,\n",
    "                                           saturation=0.8*s_jitter, hue=0.2*s_jitter)\n",
    "                ], p=0.8),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.GaussianBlur(kernel_size=max(3, image_size//20*2+1), sigma=(0.1, 2.0))\n",
    "                ], p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=NORMALIZE_STD)\n",
    "            ])\n",
    "        else: # Minimal augmentation for validation because still need two views for SimCLR loss\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(size=image_size, scale=(0.8, 1.0)), # Less aggressive crop\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomGrayscale(p=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD)\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = int(self.dataframe.iloc[idx]['age_class'])\n",
    "\n",
    "        x_i = self.transform(image)\n",
    "        x_j = self.transform(image)\n",
    "        return x_i, x_j, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b3a7fde2df46de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimCLRCoAtNetModel(nn.Module):\n",
    "    def __init__(self, coatnet_model_name, projection_out_features):\n",
    "        super().__init__()\n",
    "        #  TODO: use num_classes 0?\n",
    "        # num_classes=0 = no need for final classification layer.\n",
    "        self.encoder = timm.create_model(coatnet_model_name, pretrained=False, num_classes=0)\n",
    "\n",
    "        # Get feature dimension from CoAtNet\n",
    "        coanet_feature_dim = self.encoder.num_features\n",
    "        print(f\"CoAtNet ('{coatnet_model_name}') output feature dimension: {coanet_feature_dim}\")\n",
    "\n",
    "        # TODO: why no \"self.latent_layer = LinearLayer(128,self.num_features, True, True)\" ?\n",
    "        # Projection Head\n",
    "        self.projector = ProjectionHead(\n",
    "            in_features=coanet_feature_dim,\n",
    "            hidden_features=coanet_feature_dim,\n",
    "            out_features=projection_out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x) # Features from CoAtNet: [batch_size, coanet_feature_dim]\n",
    "        if h.ndim > 2: # Should already be [B, C] from timm with num_classes=0\n",
    "            h = torch.squeeze(h)\n",
    "        z = self.projector(h) # Projected features: [batch_size, projection_out_features]\n",
    "        return z # For SimCLR pre-training, only z is needed for loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5085342e2a6f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = UTKFaceSimCLRDataset(train_df, IMAGE_SIZE, s_jitter=0.5, is_train=True)\n",
    "val_dataset = UTKFaceSimCLRDataset(val_df, IMAGE_SIZE, is_train=False)\n",
    "test_dataset = UTKFaceSimCLRDataset(test_df, IMAGE_SIZE, is_train=False)\n",
    "\n",
    "num_workers = 2\n",
    "if platform.system() == \"Windows\":\n",
    "    num_workers = 0\n",
    "\n",
    "# drop_last=True Is needed because it caused problems with SimCLR_Loss last batch size :(\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=True, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, drop_last=True, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, drop_last=True, pin_memory=True if DEVICE.type == 'cuda' else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb69602599f1ace1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coatnet_0_224',\n",
       " 'coatnet_0_rw_224',\n",
       " 'coatnet_1_224',\n",
       " 'coatnet_1_rw_224',\n",
       " 'coatnet_2_224',\n",
       " 'coatnet_2_rw_224',\n",
       " 'coatnet_3_224',\n",
       " 'coatnet_3_rw_224',\n",
       " 'coatnet_4_224',\n",
       " 'coatnet_5_224',\n",
       " 'coatnet_bn_0_rw_224',\n",
       " 'coatnet_nano_cc_224',\n",
       " 'coatnet_nano_rw_224',\n",
       " 'coatnet_pico_rw_224',\n",
       " 'coatnet_rmlp_0_rw_224',\n",
       " 'coatnet_rmlp_1_rw2_224',\n",
       " 'coatnet_rmlp_1_rw_224',\n",
       " 'coatnet_rmlp_2_rw_224',\n",
       " 'coatnet_rmlp_2_rw_384',\n",
       " 'coatnet_rmlp_3_rw_224',\n",
       " 'coatnet_rmlp_nano_rw_224']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list available CoAtNet models\n",
    "timm.list_models(\"coatnet*\", pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46fc79aeb81f1da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoAtNet ('coatnet_0_224') output feature dimension: 768\n"
     ]
    }
   ],
   "source": [
    "COATNET_MODEL_NAME = \"coatnet_0_224\"\n",
    "PROJECTION_DIM = 128 # From SimCLR Paper\n",
    "\n",
    "model = SimCLRCoAtNetModel(\n",
    "    coatnet_model_name=COATNET_MODEL_NAME,\n",
    "    projection_out_features=PROJECTION_DIM\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52304ca3237a159a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.1\n",
    "#LEARNING_RATE = 1e-3 # AdamW\n",
    "#WEIGHT_DECAY = 1e-4 # AdamW\n",
    "\n",
    "# LEARNING_RATE = 0.2 # From notebook\n",
    "# SimCLR Paper LR Scaling: Base = 0.3 for BatchSize 256\n",
    "LEARNING_RATE = 0.3 * (BATCH_SIZE / 256.0)\n",
    "WEIGHT_DECAY = 1e-6 # From notebook and SimCLR paper\n",
    "\n",
    "EPOCHS = 50 # 5\n",
    "# 10% of total epochs for self-supervised ViT\n",
    "# Source Caron et al. (2021) \"Emerging Properties in Self-Supervised Vision Transformers\" \n",
    "WARMUP_EPOCHS = max(1, int(0.10 * EPOCHS))\n",
    "\n",
    "criterion = SimCLR_Loss(batch_size=BATCH_SIZE, temperature=TEMPERATURE).to(DEVICE)\n",
    "# AdamW does not lower the val loss with 5 epochs test\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "\n",
    "# TODO [1]: check https://github.com/hamkerlab/DL_for_practitioners/blob/c80d72b77250a7dee47a9e79182af424faffedea/04_1_ViT/04_1_WarmUpScheduler.ipynb#L71 5. Recommended Warmup Settings for Different Scenarios\n",
    "\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(\n",
    "     optimizer,\n",
    "     lambda epoch: (epoch + 1) / WARMUP_EPOCHS if WARMUP_EPOCHS > 0 else 1.0) # Ensure it stays 1.0 after warmup\n",
    "\n",
    "# Cosine Decay (NO restarts for a single run)\n",
    "scheduler_cosine_decay = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                            optimizer,\n",
    "                            T_max=EPOCHS - WARMUP_EPOCHS,\n",
    "                            eta_min=1e-6, # minimum learning rate from guidelines in [1]\n",
    "                            last_epoch=-1)\n",
    "\n",
    "combined_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer,\n",
    "                                  schedulers=[scheduler_warmup, scheduler_cosine_decay],\n",
    "                                  milestones=[WARMUP_EPOCHS])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c5ceb2da84317",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SimCLR pre-training with coatnet_0_224 for 50 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3f8c367c094814851f67bd7ca186f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50 [Train]:   0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dlfp_age_task/optimizers.py:129: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1691.)\n",
      "  next_v.mul_(momentum).add_(scaled_lr, grad)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Training Loop ---\n",
    "print(f\"Starting SimCLR pre-training with {COATNET_MODEL_NAME} for {EPOCHS} epochs...\")\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "PATH_TO_BEST_ENCODER_WEIGHTS = f'./{COATNET_MODEL_NAME}_simclr_encoder_best_val_loss.pth'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    #  Training Phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    pbar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n",
    "    for x_i, x_j, _ in pbar_train:\n",
    "        x_i, x_j = x_i.to(DEVICE), x_j.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        z_i = model(x_i)\n",
    "        z_j = model(x_j)\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        pbar_train.set_postfix({\"Loss\": loss.item()})\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    #  Validation Phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n",
    "        for x_i, x_j, _ in pbar_val:\n",
    "            x_i, x_j = x_i.to(DEVICE), x_j.to(DEVICE)\n",
    "            z_i = model(x_i)\n",
    "            z_j = model(x_j)\n",
    "            loss = criterion(z_i, z_j)\n",
    "            total_val_loss += loss.item()\n",
    "            pbar_val.set_postfix({\"Loss\": loss.item()})\n",
    "    avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Scheduler step\n",
    "    combined_scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_time_taken = (time.time() - epoch_start_time) / 60\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - LR: {current_lr:.6f} - Time: {epoch_time_taken:.2f} min\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.encoder.state_dict(), PATH_TO_BEST_ENCODER_WEIGHTS)\n",
    "        print(f\"Epoch {epoch+1}: New best SSL validation loss: {avg_val_loss:.4f}. Saved encoder to {PATH_TO_BEST_ENCODER_WEIGHTS}\")\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or (epoch + 1) == EPOCHS:\n",
    "        save_path = f'./{COATNET_MODEL_NAME}_simclr_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.encoder.state_dict(), save_path)\n",
    "        print(f\"Saved pre-trained encoder to {save_path}\")\n",
    "\n",
    "print(\"SimCLR Pre-training finished!\")\n",
    "\n",
    "# Optional: Plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SimCLR Pre-training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{COATNET_MODEL_NAME}_simclr_loss_plot.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c646e829ba0d1aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taken from https://github.com/hamkerlab/DL_for_practitioners/blob/c80d72b77250a7dee47a9e79182af424faffedea/Utils/plotting.py\n",
    "def visualize_embeddings_tsne(embeddings: np.ndarray | torch.Tensor,\n",
    "                              labels: np.ndarray | torch.Tensor,\n",
    "                              output_dir: str | None, # type hint for output_dir\n",
    "                              class_names: list[str], # type hint for class_names\n",
    "                              n_samples: int = 2000,\n",
    "                              num_components: int = 2,\n",
    "                              title_suffix: str = \"\") -> None: # Added title_suffix\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE # Ensure TSNE is imported here if not globally\n",
    "\n",
    "        # check if data is numpy\n",
    "        if torch.is_tensor(embeddings):\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "        if torch.is_tensor(labels):\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "        # Subsample if too many points\n",
    "        if len(embeddings) > n_samples:\n",
    "            print(f\"Subsampling {n_samples} out of {len(embeddings)} for t-SNE.\")\n",
    "            indices = np.random.choice(len(embeddings), n_samples, replace=False)\n",
    "            embeddings = embeddings[indices]\n",
    "            labels = labels[indices]\n",
    "\n",
    "        # Apply t-SNE\n",
    "        print(\"Applying t-SNE... (this may take a while)\")\n",
    "        #tsne = TSNE(n_components=num_components, random_state=SEED, perplexity=30, n_iter=1000, init='pca', learning_rate='auto') # Added init and lr\n",
    "        tsne = TSNE(n_components=num_components, random_state=SEED, perplexity=30, max_iter=1000, init='pca', learning_rate='auto') # Added init and lr\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "        print(\"t-SNE done.\")\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        unique_labels = np.unique(labels)\n",
    "        for class_label_val in unique_labels:\n",
    "            if int(class_label_val) < len(class_names):\n",
    "                 class_name_str = class_names[int(class_label_val)]\n",
    "            else:\n",
    "                 class_name_str = f\"Class {int(class_label_val)}\" # Fallback if class_names is too short\n",
    "\n",
    "            indices = labels == class_label_val\n",
    "            plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=class_name_str, alpha=0.7)\n",
    "\n",
    "        plt.title(f't-SNE Visualization of Encoder Embeddings {title_suffix}')\n",
    "        plt.xlabel(\"t-SNE Component 1\")\n",
    "        plt.ylabel(\"t-SNE Component 2\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        if output_dir is not None:\n",
    "            os.makedirs(output_dir, exist_ok=True) # Ensure dir exists\n",
    "            \n",
    "            base_filename = f'embeddings_tsne{title_suffix.replace(\" \", \"_\")}.png'\n",
    "            full_save_path = os.path.join(output_dir, base_filename)\n",
    "            \n",
    "            plt.savefig(full_save_path, dpi=300)\n",
    "            print(f\"t-SNE plot saved to {full_save_path}\")\n",
    "            plt.close() # Close plot if saving to file to prevent display issues in loops\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"scikit-learn not installed, skipping embedding visualization for t-SNE.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during t-SNE visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a487f3d51d8cfbd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceeding to t-SNE visualization...\n",
      "Loaded best encoder weights from ./coatnet_0_224_simclr_encoder_best_val_loss.pth for t-SNE.\n",
      "Using 3557 samples from validation set for t-SNE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c7867c46de48138889bb647ebb9281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features for t-SNE:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying t-SNE... (this may take a while)\n",
      "t-SNE done.\n",
      "t-SNE plot saved to ./tsne_plots/embeddings_tsne_after_50_Epochs_SimCLR.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProceeding to t-SNE visualization...\")\n",
    "\n",
    "# Instantiate the encoder architecture\n",
    "feature_extractor_for_tsne = timm.create_model(COATNET_MODEL_NAME, pretrained=False, num_classes=0)\n",
    "\n",
    "# Load the weights of the \"best\" encoder\n",
    "if os.path.exists(PATH_TO_BEST_ENCODER_WEIGHTS):\n",
    "    feature_extractor_for_tsne.load_state_dict(torch.load(PATH_TO_BEST_ENCODER_WEIGHTS, map_location=DEVICE))\n",
    "    print(f\"Loaded best encoder weights from {PATH_TO_BEST_ENCODER_WEIGHTS} for t-SNE.\")\n",
    "    feature_extractor_for_tsne.to(DEVICE)\n",
    "    feature_extractor_for_tsne.eval()\n",
    "\n",
    "    if val_df is not None and not val_df.empty:\n",
    "        tsne_sample_size = len(val_df)  # Number of samples for t-SNE\n",
    "        tsne_df = val_df\n",
    "        print(f\"Using {len(tsne_df)} samples from validation set for t-SNE.\")\n",
    "\n",
    "        # TODO: might need even a more minimal transform!!\n",
    "        feature_dataset = UTKFaceSimCLRDataset(tsne_df, IMAGE_SIZE, is_train=False)\n",
    "        feature_loader = DataLoader(feature_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        all_features = []\n",
    "        all_labels_for_tsne = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_i_batch, x_j_batch, labels_batch  in tqdm(feature_loader, desc=\"Extracting features for t-SNE\"):\n",
    "                images = x_i_batch.to(DEVICE)\n",
    "                 # Get 'h' from encoder?\n",
    "                features = feature_extractor_for_tsne(images)\n",
    "                all_features.append(features.cpu())\n",
    "                all_labels_for_tsne.append(labels_batch.cpu())\n",
    "\n",
    "        if all_features:\n",
    "            all_features_tensor = torch.cat(all_features, dim=0)\n",
    "            all_labels_tensor = torch.cat(all_labels_for_tsne, dim=0)\n",
    "\n",
    "            visualize_embeddings_tsne(\n",
    "                embeddings=all_features_tensor,\n",
    "                labels=all_labels_tensor,\n",
    "                output_dir=\"./tsne_plots\",\n",
    "                class_names=class_names_list,\n",
    "                n_samples=tsne_sample_size,\n",
    "                title_suffix=f\" after {EPOCHS} Epochs SimCLR\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"No features extracted for t-SNE.\")\n",
    "    else:\n",
    "        print(\"Validation DataFrame (val_df) is empty or None!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b455c56-2ce7-45e3-950a-f80aea2361a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python venv (Local)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
